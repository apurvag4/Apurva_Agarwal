{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-SRM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1v1OxT5OumkS-j89g9V0z-GxueJHm1hM-",
      "authorship_tag": "ABX9TyNyfIPwx0pye/dyfX8gy3Ys",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apurvag4/Apurva_Agarwal/blob/main/CNN_SRM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6XqEZ3NICZB"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import cv2\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_lNp-6UfYkO"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPool2D, ZeroPadding2D, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQMdvGnI5Cvh"
      },
      "source": [
        "Data_dir = \"/content/drive/My Drive/Colab Notebooks/FakeNewsDetection/fake-news-detector-master/images_train\"\n",
        "Data_dir_test = \"/content/drive/My Drive/Colab Notebooks/FakeNewsDetection/fake-news-detector-master/images_test\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wopJlbgE1EfE"
      },
      "source": [
        "CATEGORIES = ['cover', 'stego']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLPie92RIFBu"
      },
      "source": [
        "IMAGE_SIZE = 512\n",
        "NUM_CHANNELS = 1\n",
        "PIXEL_DEPTH = 255.\n",
        "NUM_LABELS = 2\n",
        "NUM_EPOCHS = 5\n",
        "batch_size = 16"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xufrNhNe582H",
        "outputId": "323799fa-56e3-400d-d51a-1b0bcc213ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  # cover and stego\n",
        "\n",
        "        path = os.path.join(Data_dir,category)  # create path to cover and stego\n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=cover 1=stego\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "                new_array = cv2.resize(img_array, (IMAGE_SIZE, IMAGE_SIZE))  # resize to normalize data size\n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e: \n",
        "                pass\n",
        "            #except OSError as e:\n",
        "            #   print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
        "            #except Exception as e:\n",
        "            #   print(\"general exception\", e, os.path.join(path,img))\n",
        "\n",
        "create_training_data()\n",
        "\n",
        "print(len(training_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 171/171 [01:30<00:00,  1.89it/s]\n",
            "100%|██████████| 239/239 [01:57<00:00,  2.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHclx2yd6ypl"
      },
      "source": [
        "random.shuffle(training_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Yp-ijwWk9N"
      },
      "source": [
        "validation_data = datagen.flow_from_directory(directory= Data_dir_test,\n",
        "                                                   target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
        "                                                   classes = ['cover', 'stego'],\n",
        "                                                   class_mode = 'binary',\n",
        "                                                   batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeFKAxtE7HN4"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for features,label in training_data:\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "\n",
        "\n",
        "# print(X[0].reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "\n",
        "X = np.array(X).reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngaxVdR0COQE"
      },
      "source": [
        "\n",
        "##### 0 - Paremeter used in the Batch-Normalization\n",
        "epsilon = 1e-4\n",
        "\n",
        "\n",
        "F_0 = tf.cast(tf.constant([[[[-1 / 12.]], [[2 / 12.]], [[-2 / 12.]], [[2 / 12.]], [[-1 / 12.]]],\n",
        "                           [[[2 / 12.]], [[-6 / 12.]], [[8 / 12.]], [[-6 / 12.]], [[2 / 12.]]],\n",
        "                           [[[-2 / 12.]], [[8 / 12.]], [[-12 / 12.]], [[8 / 12.]], [[-2 / 12.]]],\n",
        "                           [[[2 / 12.]], [[-6 / 12.]], [[8 / 12.]], [[-6 / 12.]], [[2 / 12.]]],\n",
        "                           [[[-1 / 12.]], [[2 / 12.]], [[-2 / 12.]], [[2 / 12.]], [[-1 / 12.]]]]), \"float\")\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so1QuvivPCfI"
      },
      "source": [
        "# print(X[0].reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ-T1TD4B9at"
      },
      "source": [
        "inputs = X.shape[1:]\n",
        "model = Sequential()\n",
        "model.add(Conv2D(F_0, (5, 5), strides= (1,1), padding='SAME', input_shape = inputs))\n",
        "model.add(tf.math.abs(Conv2D(8, kernel_size= (5,5) ))  # Conv1\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('Tanh'))\n",
        "model.add(AveragePooling2D( pool_size=(5,5), strides= (2,2), padding='same'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (5,5)))                              # Conv2\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('Tanh'))\n",
        "model.add(AveragePooling2D( pool_size=(5,5), strides= (2,2), padding='same'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (1,1)))                              # Conv3\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('ReLU'))\n",
        "model.add(AveragePooling2D( pool_size=(5,5), strides= (2,2), padding='same'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (1,1)))                              # Conv4\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('ReLU'))\n",
        "model.add(AveragePooling2D( pool_size=(5,5), strides= (2,2), padding='same'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (1,1)))                              # Conv5\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('ReLU'))\n",
        "model.add(AveragePooling2D( pool_size=(5,5), strides= (2,2), padding='same'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(256, (1,1)))                              # Conv6\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('ReLU'))\n",
        "model.add(GlobalAveragePooling2D( pool_size=(16,16),strides= (1,1), padding='valid'))\n",
        "\n",
        "model.add(Dense(1, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEURhIjdRrNQ"
      },
      "source": [
        "opt = SGD(learning_rate=1e-3, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit_generator(generator= (X,y), steps_per_epoch=len( training_data), epochs = 5, validation_data=validation_data, validation_steps=len(validation_data), verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s1Jsl07ZL1g"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFrZ8fVfTfYg"
      },
      "source": [
        "def plot_learningCurve(history, NUM_EPOCHS):\n",
        "  # Plot training & validation accuracy values\n",
        "  epoch_range = range(1, NUM_EPOCHS+1)\n",
        "  plt.plot(epoch_range, history.history['accuracy'])\n",
        "  plt.plot(epoch_range, history.history['val_accuracy'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(epoch_range, history.history['loss'])\n",
        "  plt.plot(epoch_range, history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMNiOFekTjWB"
      },
      "source": [
        "plot_learningCurve(history, NUM_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}